{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear = lambda:os.system('clear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataSet():# Getting the path of the dataset\n",
    "    \n",
    "    while True:\n",
    "        print(\"**************************************************\")\n",
    "        print(\"DATA SET MENU\")\n",
    "        print(\"**************************************************\")\n",
    "        print(\"1.NSL-KDD\")\n",
    "        print(\"2.IDS 2017\")\n",
    "        \n",
    "        option = input(\"Option:\")\n",
    "        \n",
    "        if option == \"1\" or option == \"2\":\n",
    "            break\n",
    "    \n",
    "    path = input(\"Path of the File:\")\n",
    "    \n",
    "    return path,option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readingData(path): #Reading the Dataset\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        option = input(\"Dataset has feature names[y/n]:\") \n",
    "        \n",
    "        if option == \"y\" or option == \"n\":\n",
    "            break\n",
    "            \n",
    "    print(\"\\nReading Dataset...\") \n",
    "        \n",
    "    if option == \"y\":\n",
    "        dataSet = pd.read_csv(path,low_memory=False)\n",
    "    \n",
    "    elif option == \"n\":\n",
    "        dataSet = pd.read_csv(path, header = None,low_memory=False)\n",
    "            \n",
    "    return dataSet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkMissing(X):#This check if the dataset given has missing values.\n",
    "    isMissing = str(X.isnull().values.any()) #Using String instead of Boolean because (\"cannot unpack non-iterable numpy.bool object\")\n",
    "    \n",
    "    if isMissing == \"True\":\n",
    "        #if data set has infinity values replace them with none\n",
    "        X = X.replace('Infinity', np.nan) #Replacing Infinity values with nan values\n",
    "           \n",
    "        missingValIndex = []\n",
    "        total = X.isnull().sum().sum()\n",
    "        percent = (total / (X.count().sum() + X.isnull().sum().sum())) * 100\n",
    "            \n",
    "        for rows in X:\n",
    "                    \n",
    "            if X[rows].isnull().sum() != 0:\n",
    "                missingValIndex.append(rows)\n",
    "        print(\"\\n\\n**************************************************\")\n",
    "        print(\"Data has missing values\")\n",
    "        print(\"**************************************************\")\n",
    "        print(\"Features with missing values:\",missingValIndex)\n",
    "        print(\"Total missing Values -> \" , total)\n",
    "        print(percent,\"%\")\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting The data we want to test for the clustering algorithms\n",
    "def gettingVariables(dataSet,dataSetOption):# If the dataset is NSL-KDD it would get the features and the labels for it and if its IDS 2017 it would take the features and the labels for it and take careof missing values.\n",
    "   \n",
    "    if dataSetOption == \"1\":\n",
    "        while True:\n",
    "            print(\"\\n\\n**************************************************\")\n",
    "            print(\"Variables Menu\")\n",
    "            print(\"**************************************************\")\n",
    "            print(\"1.Data set with categorical data oneHot encoded\")\n",
    "            print(\"2.Data set with categorical data removed\")\n",
    "            print(\"3.Data set with Risk Values replacing Server Type and Flag Features; Protocol Data oneHot encoded\")\n",
    "            option = input(\"Enter option :\")\n",
    "            \n",
    "            \n",
    "            if option == \"1\" or option == \"2\" or option == \"3\":\n",
    "                break\n",
    "            else:\n",
    "                \n",
    "                print(\"Error\\n\\n\")\n",
    "            \n",
    "        \n",
    "        if option == \"1\":\n",
    "            #Getting the Dependent and independent Variables\n",
    "            #In all the option we remove the dificulty level feature because we don't need it in our experiments\n",
    "            \n",
    "            \n",
    "            X = dataSet.iloc[:,:-2].values # Data, Get all the rows and all the clums except all the colums - 2\n",
    "            Y = dataSet.iloc[:,42].values# Labels\n",
    "            return X,Y,option\n",
    "        \n",
    "        elif option == \"2\":\n",
    "            #Removing Categorical data from the data set\n",
    "            X = dataSet.iloc[:,[0,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40]].values\n",
    "            Y = dataSet.iloc[:,42].values# Labels\n",
    "            \n",
    "            return X,Y,option\n",
    "        \n",
    "        elif option == \"3\":\n",
    "            #for later Risk Encode - Categorical features\n",
    "            X = dataSet.iloc[:,:-2].values\n",
    "            Y = dataSet.iloc[:,42].values# Labels\n",
    "            \n",
    "            return X,Y,option\n",
    "    \n",
    "\n",
    "    elif dataSetOption == \"2\":\n",
    "        #############################################################################\n",
    "        #GETTING VARIABLES\n",
    "        #############################################################################\n",
    "        missingValIndex = []\n",
    "        for rows in dataSet: #Getting features index with missing values\n",
    "            if dataSet[rows].isnull().sum() != 0:\n",
    "                    missingValIndex.append(rows)\n",
    "                \n",
    "        X = dataSet.iloc[:,:-1].values#data\n",
    "        #if names are not especified it will assign 0,1,2...n for the features name\n",
    "        X = pd.DataFrame(X,columns = [' Destination Port',' Flow Duration',' Total Fwd Packets',' Total Backward Packets','Total Length of Fwd Packets',\n",
    "                                      ' Total Length of Bwd Packets',' Fwd Packet Length Max',' Fwd Packet Length Min',' Fwd Packet Length Mean',' Fwd Packet Length Std',\n",
    "                                      'Bwd Packet Length Max',' Bwd Packet Length Min',' Bwd Packet Length Mean',' Bwd Packet Length Std','Flow Bytes/s',' Flow Packets/s',' Flow IAT Mean',\n",
    "                                      ' Flow IAT Std',' Flow IAT Max',' Flow IAT Min','Fwd IAT Total',' Fwd IAT Mean',' Fwd IAT Std',' Fwd IAT Max',' Fwd IAT Min','Bwd IAT Total',' Bwd IAT Mean',\n",
    "                                      ' Bwd IAT Std',' Bwd IAT Max',' Bwd IAT Min','Fwd PSH Flags',' Bwd PSH Flags',' Fwd URG Flags',' Bwd URG Flags',' Fwd Header Length',' Bwd Header Length','Fwd Packets/s',\n",
    "                                      ' Bwd Packets/s',' Min Packet Length',' Max Packet Length',' Packet Length Mean',' Packet Length Std',' Packet Length Variance','FIN Flag Count',' SYN Flag Count',' RST Flag Count',\n",
    "                                      ' PSH Flag Count',' ACK Flag Count',' URG Flag Count',' CWE Flag Count',' ECE Flag Count',' Down/Up Ratio',' Average Packet Size',' Avg Fwd Segment Size',' Avg Bwd Segment Size',' Fwd Header Length',\n",
    "                                      'Fwd Avg Bytes/Bulk',' Fwd Avg Packets/Bulk',' Fwd Avg Bulk Rate',' Bwd Avg Bytes/Bulk',' Bwd Avg Packets/Bulk','Bwd Avg Bulk Rate','Subflow Fwd Packets',' Subflow Fwd Bytes',' Subflow Bwd Packets',' Subflow Bwd Bytes',\n",
    "                                      'Init_Win_bytes_forward',' Init_Win_bytes_backward',' act_data_pkt_fwd',' min_seg_size_forward','Active Mean',' Active Std',' Active Max',' Active Min','Idle Mean',' Idle Std',' Idle Max',' Idle Min'])\n",
    "        Y = dataSet.iloc[:,78].values#Labels\n",
    "        \n",
    "        #############################################################################\n",
    "        #Variables Got \n",
    "        #############################################################################\n",
    "        \n",
    "    #############################################################################\n",
    "    #MANAGE MISSING DATA\n",
    "    #############################################################################   \n",
    "     \n",
    "        while True:\n",
    "            print(\"\\n\\n**************************************************\")\n",
    "            print(\"Manage Missing Values \")\n",
    "            print(\"**************************************************\")\n",
    "            print(\"1.Eliminate Catg. w/ Missing Values\")\n",
    "            print(\"2.Impute 0 for Missing Values\")\n",
    "            print(\"3.Impute Mean for Missing Values\")\n",
    "            print(\"4.Impute Median for Missing Values\")\n",
    "            print(\"5.Impute Mode for Missing Values\")\n",
    "            print(\"6.Simple Imputer\")\n",
    "            missingDataOption = input(\"Option:\")\n",
    "    \n",
    "            if missingDataOption == \"1\" or missingDataOption == \"2\" or missingDataOption == \"3\" or missingDataOption == \"4\" or missingDataOption == \"5\" or missingDataOption == \"6\":\n",
    "                break\n",
    "    \n",
    "    \n",
    "        if missingDataOption == \"1\":\n",
    "            deletedColumns = []\n",
    "            numColumns = len(X.columns)\n",
    "            #removing features with missing values\n",
    "            for row in missingValIndex:\n",
    "                deletedColumns.append(row)\n",
    "                del X[row]\n",
    "        \n",
    "            print(\"#\\n\\n########################################################################\")\n",
    "            print(\"Columns Succesfully Removed\")\n",
    "            print(len(deletedColumns),\"of\",numColumns,\"were deleted\")\n",
    "            print(\"Columns Names -> \",deletedColumns)\n",
    "            print(\"#########################################################################\")\n",
    "    \n",
    "        elif missingDataOption == \"2\":\n",
    "            #fill with 0\n",
    "            for row in missingValIndex:\n",
    "                X[row] = X[row].fillna(0)\n",
    "        \n",
    "            print(\"\\n\\n#########################################################################\")\n",
    "            print(\"Sucessfully Filled Missing Values with 0\")\n",
    "            print(\"#########################################################################\")\n",
    "    \n",
    "    \n",
    "        elif missingDataOption == \"3\":\n",
    "            #mean imputer\n",
    "            for row in missingValIndex:\n",
    "                X[row] = X[row].astype(float)\n",
    "                X[row] = X[row].fillna(X[row].mean())\n",
    "        \n",
    "            print(\"\\n\\n#########################################################################\")\n",
    "            print(\"Sucessfully Filled Missing Values with Mean\")\n",
    "            print(\"#########################################################################\")\n",
    "    \n",
    "        elif missingDataOption == \"4\":\n",
    "            #median imputer\n",
    "            for row in missingValIndex:\n",
    "                median = X[row].median()\n",
    "                X[row].fillna(median, inplace=True)\n",
    "            print(\"\\n\\n#########################################################################\")\n",
    "            print(\"Sucessfully Filled Missing Values with Median\")\n",
    "            print(\"#########################################################################\")\n",
    "    \n",
    "        elif missingDataOption == \"5\":\n",
    "            #Mode imputer\n",
    "            for row in missingValIndex:\n",
    "                X[row] = X[row].fillna(X[row].mode()[0])\n",
    "    \n",
    "            print(\"\\n\\n#########################################################################\")\n",
    "            print(\"Sucessfully Filled Missing Values with Mode \")\n",
    "            print(\"#########################################################################\")\n",
    "        \n",
    "        elif missingDataOption == \"6\": \n",
    "            from sklearn.impute import SimpleImputer\n",
    "            #\"Imputation transformer for completing missing values.\"(Univariate)\n",
    "            X = SimpleImputer(missing_values = np.nan, strategy='mean', fill_value=None, verbose=0, copy=True).fit_transform(X)          \n",
    "            print(\"\\n\\n#########################################################################\")\n",
    "            print(\"Sucessfully Imputed Simple Imputer \")\n",
    "            print(\"#########################################################################\")\n",
    "                  \n",
    "                  \n",
    "        option = \"None\" #This data does not have categorical features so dataOption is none      \n",
    "        return X,Y,option\n",
    "       \n",
    "#############################################################################\n",
    "#END OF MISSING DATA\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodingLabels(Y,dataOption,datasetOption):# Encoding the labels with multi class or binary\n",
    "    \n",
    "    if datasetOption == \"1\": #Check if the data set choosen is NSL-KDD or IDS2017\n",
    "        \n",
    "        if dataOption == \"1\" or dataOption == \"2\" or dataOption == \"3\":\n",
    "            \n",
    "            while True:\n",
    "                print(\"\\n\\n#########################################################################\")\n",
    "                print(\"Encoding Menu\")\n",
    "                print(\"#########################################################################\")\n",
    "                print(\"1.Binary true labels: normal = 0, abnormal = 1\")\n",
    "                print(\"2.Multiclass true labels: normal = 0, DoS = 1, Probe = 2, R2L = 3, U2R = 4\")\n",
    "                encodeOption = input(\"Enter option :\") \n",
    "    \n",
    "                if encodeOption == \"1\" or encodeOption == \"2\":\n",
    "                    break\n",
    "                else:\n",
    "                    \n",
    "                    print(\"Error\\n\\n\")\n",
    "    \n",
    "    \n",
    "            if encodeOption == \"1\":\n",
    "                #Binary Categories\n",
    "                attackType  = {'normal':\"normal\", 'neptune':\"abnormal\", 'warezclient':\"abnormal\", 'ipsweep':\"abnormal\",'back':\"abnormal\", 'smurf':\"abnormal\", 'rootkit':\"abnormal\",'satan':\"abnormal\", 'guess_passwd':\"abnormal\",'portsweep':\"abnormal\",'teardrop':\"abnormal\",'nmap':\"abnormal\",'pod':\"abnormal\",'ftp_write':\"abnormal\",'multihop':\"abnormal\",'buffer_overflow':\"abnormal\",'imap':\"abnormal\",'warezmaster':\"abnormal\",'phf':\"abnormal\",'land':\"abnormal\",'loadmodule':\"abnormal\",'spy':\"abnormal\",'perl':\"abnormal\"} \n",
    "                attackEncodingCluster  = {'normal':0,'abnormal':1}\n",
    "    \n",
    "                Y[:] = [attackType[item] for item in Y[:]] #Encoding the binary data\n",
    "                Y[:] = [attackEncodingCluster[item] for item in Y[:]]#Changing the names of the labels to binary labels normal and abnormal\n",
    "                return Y,encodeOption\n",
    "    \n",
    "            elif encodeOption == \"2\":\n",
    "                #4 Main Categories\n",
    "                #normal = 0\n",
    "                #DoS = 1\n",
    "                #Probe = 2\n",
    "                #R2L = 3\n",
    "                #U2R = 4\n",
    "                attackType  = {'normal': 'normal', 'neptune':'DoS', 'warezclient': 'R2L', 'ipsweep': 'Probe','back': 'DoS', 'smurf': 'DoS', 'rootkit': 'U2R','satan': 'Probe', 'guess_passwd': 'R2L','portsweep': 'Probe','teardrop': 'DoS','nmap': 'Probe','pod': 'DoS','ftp_write': 'R2L','multihop': 'R2L','buffer_overflow': 'U2R','imap': 'R2L','warezmaster': 'R2L','phf': 'R2L','land': 'DoS','loadmodule': 'U2R','spy': 'R2L','perl': 'U2R'} \n",
    "                attackEncodingCluster  = {'normal':0,'DoS':1,'Probe':2,'R2L':3, 'U2R':4} #Main Categories\n",
    "    \n",
    "                Y[:] = [attackType[item] for item in Y[:]] #Encoding the main 4 categories\n",
    "                Y[:] = [attackEncodingCluster[item] for item in Y[:]]# Changing the names of attacks into 4 main categories\n",
    "                return Y,encodeOption\n",
    "        else:\n",
    "            return Y\n",
    "    \n",
    "    \n",
    "    elif datasetOption == \"2\":#Check if the data set choosen is NSL-KDD or IDS2017\n",
    "        print(\"\\n\\n#########################################################################\")\n",
    "        print(\"Encoding Menu\")\n",
    "        print(\"#########################################################################\")\n",
    "        print(\"1.Binary true labels: normal = 0, abnormal = 1\")\n",
    "        print(\"2. Multiclass true labels: BENIGN= 0, DoS slowloris= 1, DoS Slowhttptest= 2, DoS Hulk= 3, DoS GoldenEye= 4, Heartbleed= 5\")\n",
    "        encodeOption = input(\"Enter option :\")\n",
    "\n",
    "        if encodeOption == \"1\":\n",
    "            Y = np.array(Y,dtype= object)\n",
    "            attackEncoding  = {'BENIGN': 0,'DoS slowloris': 1,'DoS Slowhttptest': 2,'DoS Hulk': 3, 'DoS GoldenEye': 4, 'Heartbleed': 5} #Main Categories\n",
    "            Y[:] = [attackEncoding[item] for item in Y[:]]# Changing the names of attacks into 4 main categories\n",
    "    \n",
    "            return Y,encodeOption\n",
    "        \n",
    "        elif encodeOption == \"2\":\n",
    "            Y = np.array(Y,dtype= object)\n",
    "            attackType  = {'BENIGN': 'normal','DoS slowloris': 'abnormal','DoS Slowhttptest': 'abnormal','DoS Hulk': 'abnormal', 'DoS GoldenEye': 'abnormal', 'Heartbleed': 'abnormal'} #Binary Categories\n",
    "            attackEncoding = {'normal': 0, 'abnormal': 1}\n",
    "            \n",
    "            Y[:] = [attackType[item] for item in Y[:]]# Changing the names of attacks into binary categories\n",
    "            Y[:] = [attackEncoding[item] for item in Y[:]]# Changing the names of attacks into binary categories\n",
    "            return Y,encodeOption\n",
    "        \n",
    "        else:\n",
    "            return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the categorical features using one hot encoding and using Main attacks categories or binary categories\n",
    "def oneHotEncodingData(X,dataOption):\n",
    "        \n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    #We use One hot encoding to pervent the machine learning to atribute the categorical data in order. \n",
    "    #What one hot encoding(ColumnTransformer) does is, it takes a column which has categorical data, \n",
    "    #which has been label encoded, and then splits the column into multiple columns.\n",
    "    #The numbers are replaced by 1s and 0s, depending on which column has what value\n",
    "    #We don't need to do a label encoded step because ColumnTransformer do one hot encode and label encode!\n",
    "    #Encoding the Independient Variable\n",
    "    if dataOption == \"1\": #Only for dataset with Categorical Data\n",
    "        transform = ColumnTransformer([(\"Servers\", OneHotEncoder(categories = \"auto\"), [1,2,3])], remainder=\"passthrough\")\n",
    "        X = transform.fit_transform(X)\n",
    "        print(\"\\n\\n#########################################################################\")\n",
    "        print(\"Data has been successfully One Hot Encoded\")\n",
    "        print(\"#########################################################################\")\n",
    "\n",
    "        return X\n",
    "    elif dataOption == \"3\": #Only for risk data, because we don't have risk values for protocol feature we do one hot encoding for only that feature and the other ones we do risk value encoding\n",
    "        transform = ColumnTransformer([(\"Servers\", OneHotEncoder(categories = \"auto\"), [1])], remainder=\"passthrough\")\n",
    "        X = transform.fit_transform(X)\n",
    "        print(\"\\n\\n#########################################################################\")\n",
    "        print(\"Data has been successfully One Hot Encoded\")\n",
    "        print(\"#########################################################################\")\n",
    "        return X\n",
    "        \n",
    "    else:\n",
    "        return X #return data with no changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def riskEncodingData(X,dataOption):#Risk encoding categorical features\n",
    "    #Manually Encoding for the attacks types only\n",
    "    if dataOption == \"3\": #if data option is risk Value\n",
    "        X = pd.DataFrame(X)\n",
    "        servers  = {'http':0.01, 'domain_u':0, 'sunrpc':1, 'smtp':0.01, 'ecr_i':0.87, 'iso_tsap':1, 'private':0.97, 'finger':0.27, 'ftp':0.26, 'telnet':0.48,'other':0.12,'discard':1, 'courier':1, 'pop_3':0.53, 'ldap':1, 'eco_i':0.8, 'ftp_data':0.06, 'klogin':1, 'auth':0.31, 'mtp':1, 'name':1, 'netbios_ns':1,'remote_job':1,'supdup':1,'uucp_path':1,'Z39_50':1,'csnet_ns':1,'uucp':1,'netbios_dgm':1,'urp_i':0,'domain':0.96,'bgp':1,'gopher':1,'vmnet':1,'systat':1,'http_443':1,'efs':1,'whois':1,'imap4':1,'echo':1,'link':1,'login':1,'kshell':1,'sql_net':1,'time':0.88,'hostnames':1,'exec':1,'ntp_u':0,'nntp':1,'ctf':1,'ssh':1,'daytime':1,'shell':1,'netstat':1,'nnsp':1,'IRC':0,'pop_2':1,'printer':1,'tim_i':0.33,'pm_dump':1,'red_i':0,'netbios_ssn':1,'rje':1,'X11':0.04,'urh_i':0,'http_8001':1,'aol':1,'http_2784':1,'tftp_u':0,'harvest':1}\n",
    "        X[2] = [servers[item] for item in X[2]]\n",
    "\n",
    "        servers_Error  = {'REJ':0.519, 'SF':0.016, 'S0':0.998, 'RSTR':0.882, 'RSTO':0.886,'SH':0.993,'S1':0.008,'RSTOS0':1,'S3':0.08,'S2':0.05,'OTH':0.729} \n",
    "        X[3] = [servers_Error[item] for item in X[3]]\n",
    "\n",
    "        print(\"\\n\\n#########################################################################\")\n",
    "        print(\"Data has been successfully risk Encoded\")\n",
    "        print(\"#########################################################################\")\n",
    "\n",
    "        return X\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        return X #return data with no changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(X):#Scalign the data with the normalize method, we scale the data to have it in the same range for the experiments\n",
    "    \n",
    "    \n",
    "\n",
    "    while True:\n",
    "            \n",
    "            decision = input(\"Scale data [y/n]:\")\n",
    "            \n",
    "            if decision == \"y\" or  decision == \"n\":\n",
    "                break\n",
    "            else:\n",
    "                \n",
    "                print(\"Error\\n\\n\")\n",
    "    \n",
    "    if decision == \"y\":\n",
    "        \n",
    "            from sklearn.preprocessing import MinMaxScaler\n",
    "            #Transforms features by scaling each feature to a given range.\n",
    "            X =  MinMaxScaler(feature_range=(0, 1)).fit_transform(X)\n",
    "            print(\"\\n\\n#########################################################################\")\n",
    "            print(\"Data has been successfully scaled.\")\n",
    "            print(\"#########################################################################\")\n",
    "            return X\n",
    "        \n",
    "    else:\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffleData(X):# currently a bug, if we do shuffleling the experiments resutls are not good, the order of the data does not affect the results\n",
    "\n",
    "    from sklearn.utils import shuffle\n",
    "    while True:\n",
    "        option = input(\"Shuffle data [y]/[n]:\")\n",
    "        \n",
    "        if option == \"y\" or option == \"n\":\n",
    "            break\n",
    "        else:\n",
    "            \n",
    "            print(\"Error\\n\\n\")\n",
    "    \n",
    "    if option == \"y\":\n",
    "        \n",
    "        X = pd.DataFrame(X)\n",
    "        X = shuffle(X)\n",
    "        X.reset_index(inplace=True,drop=True)\n",
    "        X = np.array(X)\n",
    "        \n",
    "        print(\"\\n\\n#########################################################################\")\n",
    "        print(\"Data has been successfully shuffled.\")\n",
    "        print(\"#########################################################################\")\n",
    "        return X\n",
    "    else:\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeansClustering(X,Y):#K-means algorithm \n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\\n#########################################################################\")\n",
    "        print(\"KMEANS ALGORITHM\")\n",
    "        print(\"#########################################################################\")\n",
    "              \n",
    "        nClusters = input(\"Number of clusters:\")\n",
    "        \n",
    "        try:\n",
    "            nClusters = int(nClusters)\n",
    "            \n",
    "        except ValueError:\n",
    "            \n",
    "            print(\"Error\\n\\n\")\n",
    "            \n",
    "        if type(nClusters) == int:\n",
    "            n = 0\n",
    "            clusters = []\n",
    "            \n",
    "            while n < nClusters:#Converting nCluster into an array of n clusters [n] for use it later\n",
    "                clusters.append(n)\n",
    "                n+=1\n",
    "            break\n",
    "        \n",
    "    while True:\n",
    "        init = input(\"Initialization method [k-means++,random]:\")\n",
    "        \n",
    "        if init == \"k-means++\" or init == \"random\":\n",
    "            break\n",
    "\n",
    "    print(\"\\nClustering...\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    KMEANS = KMeans(n_clusters = nClusters, init = init,max_iter = 300,n_init = 10,random_state = 0)\n",
    "    print(\"\\n\\nRun Time ->\",\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    print(\"Data Successfully Clustered\")\n",
    "    kmeans = KMEANS.fit(X)\n",
    "    Z = kmeans.labels_\n",
    "    inertia = KMEANS.inertia_\n",
    "    #Kmeans Results\n",
    "    kmeansR = pd.crosstab(Y,Z)\n",
    "    maxVal = kmeansR.idxmax()\n",
    "    \n",
    "    return Z,clusters,kmeansR,maxVal,inertia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kF1(Z,Y,maxVal,clusters):#F1 Score for Kmeans\n",
    "    from sklearn.metrics import f1_score\n",
    "    #Encoding data to F-score\n",
    "    \n",
    "    \n",
    "    # This part of the code automatically assign the max-ocurring instance in each found cluster to that specific found cluster,in order to evaluate the clustering with greater ease.\n",
    "    n = 0 # counter\n",
    "    dictionaryCluster  = {} # creating an empty dictionary \n",
    "    f1 = 0 #f1score\n",
    "    average = ''\n",
    "    \n",
    "    while n < len(clusters):# while counter < number of clusters\n",
    "        dictionaryCluster[clusters[n]] = maxVal[n] #creating key(cluster index) with value (max number of the clustering results) for every iteration\n",
    "        n+=1\n",
    "        \n",
    "    Z[:] = [dictionaryCluster[item] for item in Z[:]] # match key with the index of klabels and replace it with key value\n",
    "            \n",
    "    Y = np.array(Y,dtype = int) # Converting labels into a int array\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        average = input(\"Average Method[weighted,micro,macro,binary]:\")\n",
    "        \n",
    "        if average == \"weighted\" or average == \"micro\" or average == \"macro\" or average == 'binary':\n",
    "            break\n",
    "    #score metric   \n",
    "    f1 = f1_score(Y,Z, average = average) #Forget the labels that where not predicted and gives lables that were predicted at least once\n",
    "    \n",
    "    return f1,dictionaryCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMEANS Normal Mutial Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def kNMI(Z,Y,maxVal,clusters):\n",
    "    from sklearn.metrics import normalized_mutual_info_score\n",
    "    \n",
    "    # This part of the code automatically assign the max-ocurring instance in each found cluster to that specific found cluster,in order to evaluate the clustering with greater ease.\n",
    "    n = 0 # counter\n",
    "    dictionaryCluster  = {} # creating an empty dictionary \n",
    "    NMI = 0\n",
    "    average = ''\n",
    "    \n",
    "    while n < len(clusters):# while counter < number of clusters\n",
    "        dictionaryCluster[clusters[n]] = maxVal[n] #creating key(cluster index) with value (max number of the clustering results) for every iteration\n",
    "        n+=1\n",
    "        \n",
    "    Z[:] = [dictionaryCluster[item] for item in Z[:]] # match key with the index of klabels and replace it with key value\n",
    "    \n",
    "    Y = np.array(Y,dtype = int) #Making sure that labels are in a int array\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        average = input(\"Average Method[geometric,min,arithmetic,max]:\")\n",
    "        \n",
    "        if average == \"geometric\" or average == \"min\" or average == \"arithmetic\" or average == \"max\":\n",
    "            break\n",
    "    #Score metric \n",
    "    NMI = normalized_mutual_info_score(Y, Z, average_method = average)\n",
    "    \n",
    "    return NMI,dictionaryCluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMEANS Adjusted Random Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kARS(Z,Y,maxVal,clusters):\n",
    "    from sklearn.metrics import adjusted_rand_score\n",
    "    \n",
    "    # This part of the code automatically assign the max-ocurring instance in each found cluster to that specific found cluster,in order to evaluate the clustering with greater ease.\n",
    "    n = 0 # counter\n",
    "    dictionaryCluster  = {} # creating an empty dictionary \n",
    "    ars = 0\n",
    "    \n",
    "    while n < len(clusters):# while counter < number of clusters\n",
    "        dictionaryCluster[clusters[n]] = maxVal[n] #creating key(cluster index) with value (max number of the clustering results) for every iteration\n",
    "        n+=1\n",
    "        \n",
    "    Z[:] = [dictionaryCluster[item] for item in Z[:]] # match key with the index of klabels and replace it with key value\n",
    "    \n",
    "    Y = np.array(Y,dtype = int) #Making sure that labels are in a int array\n",
    "    \n",
    "    #score metric\n",
    "    ars = adjusted_rand_score(Y, Z)\n",
    "    \n",
    "    return ars,dictionaryCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscanClustering(X,Y):#DBSCAN algorithm\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        print(\"\\n\\n#########################################################################\")\n",
    "        print(\"DBSCAN ALGORITHM\")\n",
    "        print(\"#########################################################################\")\n",
    "              \n",
    "        epsilon = input(\"epsilon[Decimal]:\")\n",
    "        \n",
    "        try:\n",
    "            epsilon = float(epsilon)\n",
    "            \n",
    "        except ValueError:\n",
    "            \n",
    "            print(\"Enter a Decimal number\")\n",
    "            \n",
    "            \n",
    "        if type(epsilon) == float:\n",
    "            break\n",
    "        \n",
    "    while True:\n",
    "        minSamples = input(\"Min Samples[Integer]:\")\n",
    "        \n",
    "        try:\n",
    "            minSamples = int(minSamples)\n",
    "            \n",
    "        except ValueError:\n",
    "            \n",
    "            print(\"Enter a Integer Number\")\n",
    "            \n",
    "        if type(minSamples) == int:\n",
    "            break\n",
    "        \n",
    "    while True:\n",
    "        algorithm = input(\"Algorithm['auto’, ‘ball_tree’, ‘kd_tree’, 'brute']:\")\n",
    "            \n",
    "        if algorithm == \"auto\" or algorithm == \"ball_tree\" or algorithm == \"kd_tree\" or algorithm == \"brute\":\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"Error\\n\\n\")\n",
    "            \n",
    "    \n",
    "    print(\"\\nClustering...\\n\")\n",
    "\n",
    "    #Compute DBSCAN\n",
    "    start_time = time.time() \n",
    "    db = DBSCAN(eps= epsilon, min_samples = minSamples,algorithm = algorithm).fit(X)\n",
    "    print(\"\\n\\nRun Time ->\",\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    print(\"Data Successfully Clustered\")\n",
    "    \n",
    "    \n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    \n",
    "    Z = db.labels_\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters = len(set(Z))\n",
    "    n_noise_ = list(Z).count(-1)\n",
    "    \n",
    "    n = -1  # DBSCAN return index -1 cluster\n",
    "    clusters = []\n",
    "    while n + 1 < n_clusters:\n",
    "        clusters.append(n)\n",
    "        n += 1\n",
    "    \n",
    "    #DBSCAN Results\n",
    "    dbscanR = pd.crosstab(Y,Z)\n",
    "    maxVal = dbscanR.idxmax()\n",
    "    \n",
    "    return Z,clusters,n_noise_,dbscanR,maxVal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbF1(Z,Y,clusters,maxVal):#F1 score for DBSCAN\n",
    "    from sklearn.metrics import f1_score\n",
    "    #Encoding data to F-score\n",
    "    \n",
    "    # This part of the code automatically assign the max-ocurring instance in each found cluster to that specific found cluster,in order to evaluate the clustering with greater ease.\n",
    "    n = 0 # counter\n",
    "    c = -1 # - counter max Value has negative index\n",
    "    dictionaryCluster  = {} # creating an empty dictionary \n",
    "    f1 = 0\n",
    "    average = ''\n",
    "    \n",
    "    while n < len(clusters):# while counter < number of clusters\n",
    "        dictionaryCluster[clusters[n]] = maxVal[c] #creating key(cluster index) with value (max number of the clustering results) for every iteration\n",
    "        n+=1\n",
    "        c+=1\n",
    "    \n",
    "        \n",
    "    Z[:] = [dictionaryCluster[item] for item in Z[:]] # match key with the index of klabels and replace it with key value\n",
    "    \n",
    "    Y = np.array(Y,dtype = int) #Making sure that labels are in a int array\n",
    "    while True:\n",
    "        \n",
    "        average = input(\"Average Method[weighted,micro,macro]:\")\n",
    "        \n",
    "        if average == \"weighted\" or average == \"micro\" or average == \"macro\":\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            print(\"Error\\n\\n\")\n",
    "    #score metric\n",
    "    f1 = f1_score(Y,Z, average = average)\n",
    "    return f1,dictionaryCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN Mutual Info Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbNMI(Z,Y,clusters,maxVal):# Mutual info score for dbscan\n",
    "    from sklearn.metrics import normalized_mutual_info_score\n",
    "    # This part of the code automatically assign the max-ocurring instance in each found cluster to that specific found cluster,in order to evaluate the clustering with greater ease.\n",
    "    n = 0 # counter\n",
    "    c = -1 # - counter max Value has negative index\n",
    "    NMI = 0\n",
    "    dictionaryCluster  = {} # creating an empty dictionary \n",
    "    average = ''\n",
    "    \n",
    "    while n < len(clusters):# while counter < number of clusters\n",
    "        dictionaryCluster[clusters[n]] = maxVal[c] #creating key(cluster index) with value (max number of the clustering results) for every iteration\n",
    "        n+=1\n",
    "        c+=1\n",
    "    \n",
    "    Y = np.array(Y,dtype = int) #Making sure that labels are in a int array\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        average = input(\"Average Method[geometric,min,arithmetic,max]:\")\n",
    "        \n",
    "        if average == \"geometric\" or average == \"min\" or average == \"arithmetic\" or average == \"max\":\n",
    "            break\n",
    "        else:\n",
    "            \n",
    "            print(\"Error\\n\\n\")\n",
    "    #score metric\n",
    "    NMI = normalized_mutual_info_score(Y, Z, average_method= average)\n",
    "    \n",
    "    return NMI,dictionaryCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN Adjusted Random Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbARS(Z,Y,clusters,maxVal): # adjusted rand score for dbscan\n",
    "    from sklearn.metrics import adjusted_rand_score\n",
    "    \n",
    "    # This part of the code automatically assign the max-ocurring instance in each found cluster to that specific found cluster,in order to evaluate the clustering with greater ease.\n",
    "    n = 0 # counter\n",
    "    c = -1 # - counter max Value has negative index\n",
    "    ars = 0\n",
    "    dictionaryCluster  = {} # creating an empty dictionary \n",
    "    \n",
    "    while n < len(clusters):# while counter < number of clusters\n",
    "        dictionaryCluster[clusters[n]] = maxVal[c] #creating key(cluster index) with value (max number of the clustering results) for every iteration\n",
    "        n+=1\n",
    "        c+=1\n",
    "    #score metric\n",
    "    ars = adjusted_rand_score(Y,Z)\n",
    "    \n",
    "    return ars,dictionaryCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolationForest(X,Y):# isolation forest algorithm\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    \n",
    "    while True:\n",
    "        contamination = input(\"Contamination[Float 0 to 0.5]: \")\n",
    "        \n",
    "        try:\n",
    "            contamination = float(contamination)\n",
    "            \n",
    "        except ValueError:\n",
    "            \n",
    "            print(\"Enter a Number\")\n",
    "            \n",
    "        if type(contamination) == float and (contamination >= 0 and contamination <= 0.5):\n",
    "            break\n",
    "    \n",
    "    print(\"\\nClustering...\\n\")   \n",
    "    \n",
    "    start_time = time.time() \n",
    "    Z = IsolationForest(max_samples = \"auto\",behaviour = \"new\",contamination = contamination).fit_predict(X)\n",
    "    print(\"\\n\\nRun Time ->\",\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    Z = np.array(Z,dtype = object)\n",
    "    \n",
    "    ifR = pd.crosstab(Y,Z)\n",
    "    ifR = pd.DataFrame(ifR)\n",
    "    maxVal = ifR.idxmax()\n",
    "    \n",
    "    n = -1  # Isolation Forest return index -1 and 1 cluster\n",
    "    clusters = []\n",
    "    while n < len(ifR.columns):\n",
    "        clusters.append(n)\n",
    "        n += 2\n",
    "        \n",
    "    return Z,ifR,maxVal,clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolation Forest F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ifF1(Z,Y,clusters,maxVal): #f1 score for isolation forest\n",
    "    from sklearn.metrics import f1_score\n",
    "    # This part of the code automatically assign the max-ocurring instance in each found cluster to that specific found cluster,in order to evaluate the clustering with greater ease.\n",
    "    \n",
    "    n = 0 # counter\n",
    "    c = -1 # - counter max Value has negative index\n",
    "    f1 = 0\n",
    "    average = ''\n",
    "    dictionaryCluster  = {} # creating an empty dictionary \n",
    "\n",
    "    \n",
    "    while n < len(clusters): # Since we got -1 and 1 clusters , in order to assing the corrects result counter starts at -1 and it increments by 2 so it can have the 1 index of maxLOFvalue\n",
    "        dictionaryCluster[clusters[n]] = maxVal[c] \n",
    "        n+=1\n",
    "        c+=2\n",
    "        \n",
    "    Z[:] = [dictionaryCluster[item] for item in Z[:]] # match key with the index of klabels and replace it with key value\n",
    "    \n",
    "    Y = np.array(Y,dtype = int)\n",
    "    Z = np.array(Z,dtype = int)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        average = input(\"Average Method[weighted,micro,macro]:\")\n",
    "        \n",
    "        if average == \"weighted\" or average == \"micro\" or average == \"macro\":\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            print(\"Error\\n\\n\")\n",
    "    # score metric\n",
    "    f1 = f1_score(Y,Z, average = average) #[None, 'micro', 'macro', 'weighted']\n",
    "    \n",
    "    return f1,dictionaryCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Outlier Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LOF(X,Y):# Local outlier factor algorithm\n",
    "    from sklearn.neighbors import LocalOutlierFactor \n",
    "    \n",
    "    while True:\n",
    "        contamination = input(\"Contamination[Float 0 to 0.5]: \")\n",
    "        \n",
    "        try:\n",
    "            contamination = float(contamination)\n",
    "            \n",
    "        except ValueError:\n",
    "            \n",
    "            print(\"Enter a Number\")\n",
    "            \n",
    "        if type(contamination) == float and (contamination > 0 and contamination <= 0.5):\n",
    "            break\n",
    "        \n",
    "    while True:\n",
    "        algorithm = input(\"Algorithm['auto’, ‘ball_tree’, ‘kd_tree’, 'brute']:\")\n",
    "            \n",
    "        if algorithm == \"auto\" or algorithm == \"ball_tree\" or algorithm == \"kd_tree\" or algorithm == \"brute\":\n",
    "            break\n",
    "        else:\n",
    "            \n",
    "            print(\"Error\\n\\n\")\n",
    "            \n",
    "    print(\"\\nClustering...\\n\")\n",
    "    \n",
    "    start_time = time.time() \n",
    "    lof = LocalOutlierFactor(contamination = contamination,algorithm = algorithm).fit_predict(X)\n",
    "    print(\"\\n\\nRun Time ->\",\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    lofR = pd.crosstab(Y,lof)\n",
    "    maxVal = lofR.idxmax()\n",
    "    \n",
    "    \n",
    "    n = -1  # LOF return index -1 and 1 cluster\n",
    "    clusters = []\n",
    "    while n < len(lofR.columns):\n",
    "        clusters.append(n)\n",
    "        n += 2\n",
    "    \n",
    "    \n",
    "    \n",
    "    return lof,lofR,maxVal,clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Outlier Factor F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lofF1(Z,Y,clusters,maxVal): # f1 score for local outlier factor\n",
    "    from sklearn.metrics import f1_score\n",
    "    \n",
    "    # This part of the code automatically assign the max-ocurring instance in each found cluster to that specific found cluster,in order to evaluate the clustering with greater ease.\n",
    "    n = 0 # counter\n",
    "    c = -1 # - counter max Value has negative index\n",
    "    f1 = 0\n",
    "    dictionaryCluster  = {} # creating an empty dictionary \n",
    "    \n",
    "    while n < len(clusters): # Since we got -1 and 1 clusters , in order to assing the corrects result counter starts at -1 and it increments by 2 so it can have the 1 index of maxLOFvalue\n",
    "        dictionaryCluster[clusters[n]] = maxVal[c] \n",
    "        n+=1\n",
    "        c+=2\n",
    "        \n",
    "    Z[:] = [dictionaryCluster[item] for item in Z[:]] # match key with the index of klabels and replace it with key value\n",
    "    Y = np.array(Y,dtype = int)\n",
    "    Z = np.array(Z,dtype = int)\n",
    "    while True:\n",
    "        \n",
    "        average = input(\"Average Method[weighted,None,micro,macro]:\")\n",
    "        \n",
    "        if average == \"weighted\" or average == \"micro\" or average == \"macro\" or average == \"None\":\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            print(\"Error\\n\\n\")\n",
    "    f1 = f1_score(Y,Z, average = average) #[None, 'micro', 'macro', 'weighted']\n",
    "    \n",
    "    return f1,dictionaryCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "DATA SET MENU\n",
      "**************************************************\n",
      "1.NSL-KDD\n",
      "2.IDS 2017\n",
      "Option:1\n",
      "Path of the File:C:\\Users\\Long\\Desktop\\clustering-based-anomaly-detection-master\\clustering-based-anomaly-detection-master\\Dataset\\KDDTrain+.csv\n",
      "Dataset has feature names[y/n]:y\n",
      "\n",
      "Reading Dataset...\n",
      "\n",
      "\n",
      "**************************************************\n",
      "Variables Menu\n",
      "**************************************************\n",
      "1.Data set with categorical data oneHot encoded\n",
      "2.Data set with categorical data removed\n",
      "3.Data set with Risk Values replacing Server Type and Flag Features; Protocol Data oneHot encoded\n",
      "Enter option :1\n",
      "\n",
      "\n",
      "#########################################################################\n",
      "Encoding Menu\n",
      "#########################################################################\n",
      "1.Binary true labels: normal = 0, abnormal = 1\n",
      "2.Multiclass true labels: normal = 0, DoS = 1, Probe = 2, R2L = 3, U2R = 4\n",
      "Enter option :2\n",
      "\n",
      "\n",
      "#########################################################################\n",
      "Data has been successfully One Hot Encoded\n",
      "#########################################################################\n",
      "Scale data [y/n]:y\n",
      "\n",
      "\n",
      "#########################################################################\n",
      "Data has been successfully scaled.\n",
      "#########################################################################\n",
      "Shuffle data [y]/[n]:n\n",
      "\n",
      "\n",
      "#########################################################################\n",
      "Algorithm Menu\n",
      "#########################################################################\n",
      "1.Kmeans\n",
      "2.Dbscan\n",
      "3.Isolation Forest\n",
      "4.Local Factor Outlier\n",
      "option:2\n",
      "\n",
      "\n",
      "#########################################################################\n",
      "DBSCAN ALGORITHM\n",
      "#########################################################################\n",
      "epsilon[Decimal]:0.5\n",
      "Min Samples[Integer]:700\n",
      "Algorithm['auto’, ‘ball_tree’, ‘kd_tree’, 'brute']:brute\n",
      "\n",
      "Clustering...\n",
      "\n",
      "\n",
      "\n",
      "Run Time -> --- 432.6451756954193 seconds ---\n",
      "Data Successfully Clustered\n",
      "#########################################################################\n",
      "DBSCAN RESULTS\n",
      "\n",
      "\n",
      "Clusters ->  [-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17] \n",
      "\n",
      "col_0    -1      0      1     2     3     4     5     6     7     8     9   \\\n",
      "row_0                                                                        \n",
      "0      12196      0  33908     0  2896  1137     0     0  7822     0  2593   \n",
      "1      24291  12705    838  2227     0     0     0  1008     0  1022     0   \n",
      "2       7243      0      0     5     0    24  3472     0     0     0     1   \n",
      "3        128      0      0     0   566     0     0     0     0     0     0   \n",
      "4         45      0      0     0     7     0     0     0     0     0     0   \n",
      "\n",
      "col_0    10    11   12    13   14    15   16    17  \n",
      "row_0                                               \n",
      "0      3406  1547  677     0    0     0    0  1160  \n",
      "1         0     0    0  1897    0  1062  877     0  \n",
      "2         6     0    0     0  905     0    0     0  \n",
      "3         0     0  301     0    0     0    0     0  \n",
      "4         0     0    0     0    0     0    0     0   \n",
      "\n",
      "\n",
      "Noise ->  43903\n",
      "Max True Label \n",
      "\n",
      " col_0\n",
      "-1     1\n",
      " 0     1\n",
      " 1     0\n",
      " 2     1\n",
      " 3     0\n",
      " 4     0\n",
      " 5     2\n",
      " 6     1\n",
      " 7     0\n",
      " 8     1\n",
      " 9     0\n",
      " 10    0\n",
      " 11    0\n",
      " 12    0\n",
      " 13    1\n",
      " 14    2\n",
      " 15    1\n",
      " 16    1\n",
      " 17    0\n",
      "dtype: int64\n",
      "#########################################################################\n",
      "\n",
      "\n",
      "#########################################################################\n",
      "Dscan Score Metrics Menu\n",
      "#########################################################################\n",
      "1.F1 Score\n",
      "2.Normalized Mutual Info Score\n",
      "3.Adjusted Rand Score\n",
      "option:1\n",
      "Average Method[weighted,micro,macro]:micro\n",
      "\n",
      "\n",
      "#########################################################################\n",
      "Cluster Matchings by Maximun Intersection[Found: True] ->  {-1: 1, 0: 1, 1: 0, 2: 1, 3: 0, 4: 0, 5: 2, 6: 1, 7: 0, 8: 1, 9: 0, 10: 0, 11: 0, 12: 0, 13: 1, 14: 2, 15: 1, 16: 1, 17: 0}\n",
      "DBSCAN F1 Score ->  0.8304385101451116\n",
      "#########################################################################\n",
      "Try another Clustering Algorithm[y/n]:y\n",
      "\n",
      "\n",
      "#########################################################################\n",
      "Algorithm Menu\n",
      "#########################################################################\n",
      "1.Kmeans\n",
      "2.Dbscan\n",
      "3.Isolation Forest\n",
      "4.Local Factor Outlier\n",
      "option:1\n",
      "\n",
      "\n",
      "#########################################################################\n",
      "KMEANS ALGORITHM\n",
      "#########################################################################\n",
      "Number of clusters:50\n",
      "Initialization method [k-means++,random]:random\n",
      "\n",
      "Clustering...\n",
      "\n",
      "\n",
      "\n",
      "Run Time -> --- 0.0 seconds ---\n",
      "Data Successfully Clustered\n",
      "#########################################################################\n",
      "KMEANS RESULTS\n",
      "\n",
      "\n",
      "Clusters ->  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49] \n",
      "\n",
      "Inertia ->  55728.927148554045\n",
      "col_0    0     1    2    3    4     5     6     7     8    9   ...   40    41  \\\n",
      "row_0                                                          ...              \n",
      "0      1046     0  604  287  282  7428  1205  2910     7  942  ...    0     0   \n",
      "1       450  1456    3  122    5     0     0   112  1095    0  ...  567  7527   \n",
      "2         1     4    0  620   16     0     3     0     4    0  ...    1    43   \n",
      "3         0     0    0    0    0     0     0     0     0  566  ...    0     0   \n",
      "4         0     0    0    0    0     0     0     0     0   29  ...    0     0   \n",
      "\n",
      "col_0    42    43    44    45   46    47    48    49  \n",
      "row_0                                                 \n",
      "0         9   112  4471  2054  363    16  1206  1676  \n",
      "1      3089  1216     0     0    0   892     0     3  \n",
      "2        41    78     0   135    0  1032    39     0  \n",
      "3         1    44     0    10    0     0     0     0  \n",
      "4         0     0     0    17    0     0     2     0  \n",
      "\n",
      "[5 rows x 50 columns] \n",
      "\n",
      "\n",
      "Max True Label \n",
      "\n",
      " col_0\n",
      "0     0\n",
      "1     1\n",
      "2     0\n",
      "3     2\n",
      "4     0\n",
      "5     0\n",
      "6     0\n",
      "7     0\n",
      "8     1\n",
      "9     0\n",
      "10    0\n",
      "11    0\n",
      "12    0\n",
      "13    0\n",
      "14    0\n",
      "15    0\n",
      "16    0\n",
      "17    0\n",
      "18    2\n",
      "19    0\n",
      "20    2\n",
      "21    0\n",
      "22    0\n",
      "23    0\n",
      "24    1\n",
      "25    1\n",
      "26    0\n",
      "27    0\n",
      "28    1\n",
      "29    0\n",
      "30    2\n",
      "31    2\n",
      "32    1\n",
      "33    1\n",
      "34    1\n",
      "35    0\n",
      "36    0\n",
      "37    1\n",
      "38    0\n",
      "39    0\n",
      "40    1\n",
      "41    1\n",
      "42    1\n",
      "43    1\n",
      "44    0\n",
      "45    0\n",
      "46    0\n",
      "47    2\n",
      "48    0\n",
      "49    0\n",
      "dtype: int64\n",
      "#########################################################################\n",
      "\n",
      "\n",
      "#########################################################################\n",
      "Kmeans Score Metrics Menu\n",
      "#########################################################################\n",
      "1.F1 Score\n",
      "2.Normalized Mutual Info Score\n",
      "3.Adjusted Rand Score\n",
      "option:1\n",
      "Average Method[weighted,micro,macro,binary]:binary\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 88\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kScoreOption \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m#########################################################################\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m#F1 Score\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m     kmeansF1,clusterAssigned \u001b[38;5;241m=\u001b[39m \u001b[43mkF1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mklabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmaxKvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkClusters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m#########################################################################\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCluster Matchings by Maximun Intersection[Found: True] -> \u001b[39m\u001b[38;5;124m\"\u001b[39m,clusterAssigned)\n",
      "Cell \u001b[1;32mIn[13], line 27\u001b[0m, in \u001b[0;36mkF1\u001b[1;34m(Z, Y, maxVal, clusters)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#score metric   \u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m f1 \u001b[38;5;241m=\u001b[39m \u001b[43mf1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#Forget the labels that where not predicted and gives lables that were predicted at least once\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f1,dictionaryCluster\n",
      "File \u001b[1;32m~\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1146\u001b[0m, in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1011\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf1_score\u001b[39m(\n\u001b[0;32m   1012\u001b[0m     y_true,\n\u001b[0;32m   1013\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1020\u001b[0m ):\n\u001b[0;32m   1021\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \n\u001b[0;32m   1023\u001b[0m \u001b[38;5;124;03m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;124;03m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfbeta_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1287\u001b[0m, in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfbeta_score\u001b[39m(\n\u001b[0;32m   1159\u001b[0m     y_true,\n\u001b[0;32m   1160\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1167\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1168\u001b[0m ):\n\u001b[0;32m   1169\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m \n\u001b[0;32m   1171\u001b[0m \u001b[38;5;124;03m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;124;03m    array([0.71..., 0.        , 0.        ])\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1287\u001b[0m     _, _, f, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf-score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "File \u001b[1;32m~\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1573\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m beta \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1572\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta should be >=0 in the F-beta score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1573\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1575\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1576\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1391\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1389\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1390\u001b[0m             average_options\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1392\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget is \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m but average=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1393\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoose another average setting, one of \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (y_type, average_options)\n\u001b[0;32m   1394\u001b[0m         )\n\u001b[0;32m   1395\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pos_label \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1396\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1397\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNote that pos_label (set to \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) is ignored when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage != \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m). You may use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1401\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1402\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
     ]
    }
   ],
   "source": [
    "clear()\n",
    "#Calling the functions\n",
    "\n",
    "##########################################################################\n",
    "path,dataSetOption = getDataSet()\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "dataSet = readingData(path)\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "dataSet = checkMissing(dataSet)\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "data,labels,dataOption = gettingVariables(dataSet,dataSetOption) #Getting the Data we want to use for the algorithms\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "try:\n",
    "    labels,encodeOption = encodingLabels(labels,dataOption,dataSetOption) #Encoding the true labels\n",
    "except ValueError:\n",
    "    labels = encodingLabels(labels,dataOption,dataSetOption) #Encoding the true labels\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "data = riskEncodingData(data,dataOption)\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "data = oneHotEncodingData(data,dataOption) #One hot Encode with the complete data\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "data = scaling(data)\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "data = shuffleData(data)\n",
    "#########################################################################\n",
    "\n",
    "#This menu is a option to run diferrent algorithms with the same preproceced data witouth the need of running all the code from 0 to make another experiment.\n",
    "while True:  \n",
    "    while True:\n",
    "        print(\"\\n\\n#########################################################################\")\n",
    "        print(\"Algorithm Menu\")\n",
    "        print(\"#########################################################################\")\n",
    "        \n",
    "        print(\"1.Kmeans\")\n",
    "        print(\"2.Dbscan\")\n",
    "        print(\"3.Isolation Forest\")\n",
    "        print(\"4.Local Factor Outlier\")\n",
    "        \n",
    "        algorithmOption = input(\"option:\")\n",
    "        \n",
    "        if algorithmOption == \"1\" or algorithmOption == \"2\" or algorithmOption == \"3\" or algorithmOption == \"4\":\n",
    "                break\n",
    "        else:\n",
    "            \n",
    "            print(\"Error\\n\\n\")\n",
    "\n",
    "    \n",
    "    if algorithmOption == \"1\":\n",
    "        #########################################################################\n",
    "        #KMEANS\n",
    "        klabels,kClusters,kmeansR,maxKvalue,inertia = kmeansClustering(data,labels)\n",
    "        print(\"#########################################################################\")\n",
    "        print(\"KMEANS RESULTS\\n\\n\")\n",
    "        print(\"Clusters -> \",kClusters,\"\\n\")\n",
    "        print(\"Inertia -> \",inertia)\n",
    "        print(kmeansR,\"\\n\\n\")\n",
    "        print(\"Max True Label\",\"\\n\\n\",maxKvalue)\n",
    "        print(\"#########################################################################\")\n",
    "        #########################################################################\n",
    "        print(\"\\n\\n#########################################################################\")\n",
    "        print(\"Kmeans Score Metrics Menu\")\n",
    "        print(\"#########################################################################\")\n",
    "        \n",
    "        while True:\n",
    "            print(\"1.F1 Score\")\n",
    "            print(\"2.Normalized Mutual Info Score\")\n",
    "            print(\"3.Adjusted Rand Score\")\n",
    "        \n",
    "            kScoreOption = input(\"option:\")\n",
    "            \n",
    "            if kScoreOption == \"1\" or kScoreOption == \"2\" or kScoreOption == \"3\":\n",
    "                break\n",
    "            else:\n",
    "                \n",
    "                print(\"Error\\n\\n\")\n",
    "     \n",
    "        if kScoreOption == \"1\":\n",
    "            #########################################################################\n",
    "            #F1 Score\n",
    "            kmeansF1,clusterAssigned = kF1(klabels,labels,maxKvalue,kClusters)\n",
    "            print(\"\\n\\n#########################################################################\")\n",
    "            print(\"Cluster Matchings by Maximun Intersection[Found: True] -> \",clusterAssigned)\n",
    "            print(\"KMEANS F1 Score -> \",kmeansF1)\n",
    "            print(\"#########################################################################\")\n",
    "            #########################################################################\n",
    "        \n",
    "        elif kScoreOption == \"2\":\n",
    "            #########################################################################\n",
    "            kmeansNMI,clusterAssigned = kNMI(klabels,labels,maxKvalue,kClusters)\n",
    "            print(\"\\n\\n#########################################################################\")\n",
    "            print(\"Cluster Matchings by Maximun Intersection[Found: True] -> \",clusterAssigned)\n",
    "            print(\"KMEANS Normalized Mutual Info Score -> \",kmeansNMI)\n",
    "            print(\"#########################################################################\")\n",
    "            #########################################################################\n",
    "    \n",
    "        elif kScoreOption == \"3\":\n",
    "            \n",
    "            #########################################################################\n",
    "            kmeansARS,clusterAssigned = kARS(klabels,labels,maxKvalue,kClusters)\n",
    "            print(\"\\n\\n#########################################################################\")\n",
    "            print(\"Cluster Matchings by Maximun Intersection[Found: True] -> \",clusterAssigned)\n",
    "            print(\"KMEANS Adjusted Rand Score -> \",kmeansARS)\n",
    "            print(\"#########################################################################\")\n",
    "            #########################################################################\n",
    "            \n",
    "    elif algorithmOption == \"2\":\n",
    "        #########################################################################\n",
    "        #DBSCAN\n",
    "        dblabels,dbClusters,nNoises,dbscanR,maxDBvalue = dbscanClustering(data,labels) \n",
    "        print(\"#########################################################################\")\n",
    "        print(\"DBSCAN RESULTS\\n\\n\")\n",
    "        print(\"Clusters -> \",dbClusters,\"\\n\")\n",
    "        print(dbscanR,\"\\n\\n\")\n",
    "        print(\"Noise -> \",nNoises)\n",
    "        print(\"Max True Label\",\"\\n\\n\",maxDBvalue)\n",
    "        print(\"#########################################################################\")\n",
    "        #########################################################################\n",
    "        print(\"\\n\\n#########################################################################\")\n",
    "        print(\"Dscan Score Metrics Menu\")\n",
    "        print(\"#########################################################################\")\n",
    "        print(\"1.F1 Score\")\n",
    "        print(\"2.Normalized Mutual Info Score\")\n",
    "        print(\"3.Adjusted Rand Score\")\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            dbScoreOption = input(\"option:\")\n",
    "            \n",
    "            if dbScoreOption == \"1\" or dbScoreOption == \"2\" or dbScoreOption == \"3\":\n",
    "                break\n",
    "            else:\n",
    "                \n",
    "                print(\"Error\\n\\n\")\n",
    "    \n",
    "        if dbScoreOption == \"1\":\n",
    "            #########################################################################\n",
    "            #F1 Score dbscan\n",
    "            dbscanF1,clusterAssigned = dbF1(dblabels,labels,dbClusters,maxDBvalue)\n",
    "            print(\"\\n\\n#########################################################################\")\n",
    "            print(\"Cluster Matchings by Maximun Intersection[Found: True] -> \",clusterAssigned)\n",
    "            print(\"DBSCAN F1 Score -> \",dbscanF1)\n",
    "            print(\"#########################################################################\")\n",
    "            #########################################################################\n",
    "            \n",
    "        elif dbScoreOption == \"2\":\n",
    "            #########################################################################\n",
    "            dbscanNMI,clusterAssigned = dbNMI(dblabels,labels,dbClusters,maxDBvalue)\n",
    "            print(\"\\n\\n#########################################################################\")\n",
    "            print(\"Cluster Matchings by Maximun Intersection[Found: True] -> \",clusterAssigned)\n",
    "            print(\"DBSCAN Normalized Mutual Info Score -> \",dbscanNMI)\n",
    "            print(\"#########################################################################\")\n",
    "            #########################################################################\n",
    "            \n",
    "        elif dbScoreOption == \"3\":\n",
    "            #########################################################################\n",
    "            dbscanARS,clusterAssigned = dbARS(dblabels,labels,dbClusters,maxDBvalue)\n",
    "            print(\"\\n\\n#########################################################################\")\n",
    "            print(\"Cluster Matchings by Maximun Intersection[Found: True] -> \",clusterAssigned)\n",
    "            print(\"DBSCAN Adjusted Rand Score -> \",dbscanARS)\n",
    "            print(\"#########################################################################\")\n",
    "            #########################################################################\n",
    "        \n",
    "        \n",
    "    elif algorithmOption == \"3\":\n",
    "        #########################################################################\n",
    "        ifLabels,ifR,MaxIfVal,ifNclusters = isolationForest(data,labels)\n",
    "        print(\"#########################################################################\")\n",
    "        print(\"Isolation Forest RESULTS\\n\\n\")\n",
    "        print(\"Clusters -> \",ifNclusters,\"\\n\")\n",
    "        print(ifR,\"\\n\\n\")\n",
    "        print(\"Max True Label\",\"\\n\\n\",MaxIfVal)\n",
    "        print(\"#########################################################################\")\n",
    "        #########################################################################\n",
    "        print(\"\\n\\n#########################################################################\")\n",
    "        print(\"Isolation Forest Score Metrics Menu\")\n",
    "        print(\"#########################################################################\")\n",
    "        print(\"1.F1 Score\")\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            ifScoreOption = input(\"option:\")\n",
    "            \n",
    "            if ifScoreOption == \"1\":\n",
    "                break\n",
    "            else:\n",
    "                \n",
    "                print(\"Error\\n\\n\")\n",
    "        \n",
    "        if ifScoreOption == \"1\":\n",
    "            \n",
    "            ##########################################################################\n",
    "            isolationForestF1,clusterAssigned = ifF1(ifLabels,labels,ifNclusters,MaxIfVal)\n",
    "            print(\"\\n\\n#########################################################################\")\n",
    "            print(\"Cluster Matchings by Maximun Intersection[Found: True] -> \",clusterAssigned)\n",
    "            print(\"Isolation Forest F1 Score -> \",isolationForestF1)\n",
    "            print(\"#########################################################################\")\n",
    "            ##########################################################################\n",
    "        \n",
    "    elif algorithmOption == \"4\":\n",
    "        #########################################################################\n",
    "        LOFlabels,lofR,maxLOFvalue,lofClusters = LOF(data,labels)\n",
    "        print(\"#########################################################################\")\n",
    "        print(\"Local Outlier Factor RESULTS\\n\\n\")\n",
    "        print(\"Clusters -> \",lofClusters,\"\\n\")\n",
    "        print(lofR,\"\\n\\n\")\n",
    "        print(\"Max True Label\",\"\\n\\n\",maxLOFvalue)\n",
    "        print(\"#########################################################################\")\n",
    "        #########################################################################\n",
    "        print(\"\\n\\n#########################################################################\")\n",
    "        print(\"LOF Score Metrics Menu\")\n",
    "        print(\"#########################################################################\")\n",
    "        print(\"1.F1 Score\")\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            lofScoreOption = input(\"option:\")\n",
    "            \n",
    "            if lofScoreOption == \"1\":\n",
    "                break\n",
    "            else:\n",
    "                \n",
    "                print(\"Error\\n\\n\")\n",
    "        \n",
    "        if lofScoreOption == \"1\":\n",
    "            \n",
    "            ##########################################################################\n",
    "            LOFf1,clusterAssigned = lofF1(LOFlabels,labels,lofClusters,maxLOFvalue)\n",
    "            print(\"\\n\\n#########################################################################\")\n",
    "            print(\"Cluster Matchings by Maximun Intersection[Found: True] -> \",clusterAssigned)\n",
    "            print(\"LOF F1 Score -> \",LOFf1)\n",
    "            print(\"#########################################################################\")\n",
    "            ##########################################################################\n",
    "                \n",
    "    while True: # If the user want to Make a new clustering algorithm test\n",
    "        \n",
    "        decision = input(\"Try another Clustering Algorithm[y/n]:\")\n",
    "        \n",
    "        if decision == \"y\" or  decision == \"n\":\n",
    "            break\n",
    "        else:\n",
    "            \n",
    "            print(\"Error\\n\\n\")\n",
    "    \n",
    "    \n",
    "    if decision == \"n\":\n",
    "        break\n",
    "    \n",
    "    else:\n",
    "        clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "923px",
    "left": "328px",
    "right": "20px",
    "top": "9px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
